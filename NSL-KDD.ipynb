{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from scipy import sparse\n",
    "import pickle\n",
    "\n",
    "# Load and preprocess data\n",
    "data = pd.read_csv(r'P:\\project ides for final ptoject\\project\\IDS_Federated learning model\\model\\cleaned_NSL_KDD_dataset.csv', low_memory=False)\n",
    "columns_to_keep = ['duration', 'protocol_type', 'service', 'src_bytes', 'dst_bytes', 'num_failed_logins', 'logged_in', 'attack']\n",
    "data_filtered = data[columns_to_keep]\n",
    "X = data_filtered.drop(columns=['attack'])\n",
    "y = data_filtered['attack'].apply(lambda x: 0 if x == 'normal' else 1)\n",
    "\n",
    "categorical_columns = ['protocol_type', 'service', 'logged_in']\n",
    "numerical_columns = X.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', StandardScaler(), numerical_columns),\n",
    "    ('cat', OneHotEncoder(sparse_output=False), categorical_columns)\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "if sparse.issparse(X_train):\n",
    "    X_train = X_train.toarray()\n",
    "if sparse.issparse(X_test):\n",
    "    X_test = X_test.toarray()\n",
    "\n",
    "# Activation functions and loss\n",
    "def leaky_relu(Z, alpha=0.01):\n",
    "    return np.where(Z > 0, Z, alpha * Z)\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "# Initialize weights and biases\n",
    "def initialize_weights(layers):\n",
    "    np.random.seed(42)\n",
    "    weights = []\n",
    "    biases = []\n",
    "    for i in range(1, len(layers)):\n",
    "        weight = np.random.randn(layers[i-1], layers[i]) * np.sqrt(2 / layers[i-1])\n",
    "        bias = np.zeros((1, layers[i]))\n",
    "        weights.append(weight)\n",
    "        biases.append(bias)\n",
    "    return weights, biases\n",
    "\n",
    "# Forward propagation\n",
    "def forward_propagation(X, weights, biases):\n",
    "    A = X\n",
    "    caches = []\n",
    "    for i in range(len(weights) - 1):  # Loop through all layers except the final one\n",
    "        Z = np.dot(A, weights[i]) + biases[i]\n",
    "        A = leaky_relu(Z)\n",
    "        caches.append((A, Z))\n",
    "    Z_final = np.dot(A, weights[-1]) + biases[-1]\n",
    "    A_final = sigmoid(Z_final)\n",
    "    caches.append((A_final, Z_final))\n",
    "    return A_final, caches\n",
    "\n",
    "# Backward propagation\n",
    "def backward_propagation(X_batch, y_batch, caches, weights, biases, learning_rate):\n",
    "    m = X_batch.shape[0]\n",
    "    A_final, Z_final = caches[-1]\n",
    "    y_batch = y_batch.to_numpy().reshape(-1, 1)\n",
    "    dZ_final = A_final - y_batch\n",
    "    dW_final = np.dot(caches[-2][0].T, dZ_final) / m\n",
    "    db_final = np.sum(dZ_final, axis=0, keepdims=True) / m\n",
    "    weights[-1] -= learning_rate * dW_final\n",
    "    biases[-1] -= learning_rate * db_final\n",
    "\n",
    "    dA = dZ_final\n",
    "    for i in reversed(range(len(weights) - 1)):\n",
    "        A_prev, Z_prev = caches[i]\n",
    "        dZ = dA.dot(weights[i+1].T) * (Z_prev > 0)\n",
    "        dW = np.dot(caches[i-1][0].T, dZ) / m if i > 0 else np.dot(X_batch.T, dZ) / m\n",
    "        db = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "        weights[i] -= learning_rate * dW\n",
    "        biases[i] -= learning_rate * db\n",
    "        dA = dZ\n",
    "    return weights, biases\n",
    "\n",
    "# Training function\n",
    "def train(X_train, y_train, layers, epochs=30, batch_size=64, learning_rate=0.001):\n",
    "    weights, biases = initialize_weights(layers)\n",
    "    m = X_train.shape[0]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, m, batch_size):\n",
    "            X_batch = X_train[i:i + batch_size]\n",
    "            y_batch = y_train[i:i + batch_size]\n",
    "            y_pred, caches = forward_propagation(X_batch, weights, biases)\n",
    "            weights, biases = backward_propagation(X_batch, y_batch, caches, weights, biases, learning_rate)\n",
    "\n",
    "        # Evaluate accuracy\n",
    "        y_train_pred, _ = forward_propagation(X_train, weights, biases)\n",
    "        y_test_pred, _ = forward_propagation(X_test, weights, biases)\n",
    "        train_accuracy = np.mean((y_train_pred > 0.5).astype(int) == y_train.values.reshape(-1, 1))\n",
    "        test_accuracy = np.mean((y_test_pred > 0.5).astype(int) == y_test.values.reshape(-1, 1))\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Train Accuracy: {train_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    # Save final weights and biases to a pickle file\n",
    "    final_model_data = {\n",
    "        'weights': weights,\n",
    "        'biases': biases\n",
    "    }\n",
    "    with open('model_weights_and_biases2.pickle', 'wb') as f:\n",
    "        pickle.dump(final_model_data, f)\n",
    "    print(\"Final model weights and biases saved to 'model_weights_and_biases2.pickle'.\")\n",
    "\n",
    "# Define layer structure\n",
    "layers = [X_train.shape[1], 512, 256, 128, 1]  # Input layer, hidden layers, output layer\n",
    "\n",
    "# Train the model\n",
    "train(X_train, y_train, layers, epochs=30, batch_size=64, learning_rate=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the saved model\n",
    "with open('model_weights_and_biases2.pickle', 'rb') as f:\n",
    "    model_data = pickle.load(f)\n",
    "\n",
    "weights = model_data['weights']\n",
    "biases = model_data['biases']\n",
    "\n",
    "# Predict function\n",
    "def predict(X, weights, biases):\n",
    "    y_pred, _ = forward_propagation(X, weights, biases)\n",
    "    return (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Get predictions for the test set\n",
    "y_test_pred_binary = predict(X_test, weights, biases)\n",
    "\n",
    "# Metrics calculation\n",
    "accuracy = accuracy_score(y_test, y_test_pred_binary)\n",
    "precision = precision_score(y_test, y_test_pred_binary)\n",
    "recall = recall_score(y_test, y_test_pred_binary)\n",
    "f1 = f1_score(y_test, y_test_pred_binary)\n",
    "\n",
    "# Print metrics\n",
    "print(\"Metrics on Test Set:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred_binary)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Normal\", \"Attack\"], yticklabels=[\"Normal\", \"Attack\"])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
