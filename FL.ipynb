{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Function to load the .pickle file\n",
    "def load_pickle(file_name):\n",
    "    with open(file_name, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "# Function to inspect the structure of the pickle file\n",
    "def inspect_pickle_structure(model_data):\n",
    "    print(f\"Keys in model_data: {model_data.keys()}\")\n",
    "    if 'parameters' in model_data:\n",
    "        print(f\"Keys in 'parameters': {model_data['parameters'].keys()}\")\n",
    "    if 'initial_weights' in model_data:\n",
    "        print(f\"Length of 'initial_weights': {len(model_data['initial_weights'])}\")\n",
    "    if 'initial_biases' in model_data:\n",
    "        print(f\"Length of 'initial_biases': {len(model_data['initial_biases'])}\")\n",
    "\n",
    "# Function to create a model from the pickle file data\n",
    "def create_model_from_pickle(model_data):\n",
    "    # Inspect the model data structure to determine the correct keys\n",
    "    if 'parameters' not in model_data:\n",
    "        print(\"Error: 'parameters' key not found in model_data\")\n",
    "        return None\n",
    "\n",
    "    # Get the layer sizes and other parameters\n",
    "    try:\n",
    "        layer_sizes = model_data['parameters']['layer_size']\n",
    "    except KeyError:\n",
    "        print(\"Error: 'layer_size' key not found in 'parameters'\")\n",
    "        return None\n",
    "\n",
    "    weights = model_data['initial_weights']\n",
    "    biases = model_data.get('initial_biases', None)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Add layers to the model based on the extracted sizes and weights\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        # Create the layers\n",
    "        if i == 0:\n",
    "            model.add(layers.Dense(layer_sizes[i + 1], input_dim=layer_sizes[i], activation='relu'))\n",
    "        else:\n",
    "            model.add(layers.Dense(layer_sizes[i + 1], activation='relu'))\n",
    "\n",
    "        # Check and convert the weights and biases into correct numpy arrays\n",
    "        try:\n",
    "            weight_array = np.array(weights[i]) if not isinstance(weights[i], np.ndarray) else weights[i]\n",
    "            # Ensure weight_array is two-dimensional and has the correct shape\n",
    "            if weight_array.ndim != 2 or weight_array.shape != (layer_sizes[i], layer_sizes[i + 1]):\n",
    "                print(f\"Warning: Weight shape mismatch at layer {i}, expected shape ({layer_sizes[i]}, {layer_sizes[i + 1]}) but got {weight_array.shape}\")\n",
    "                weight_array = np.zeros((layer_sizes[i], layer_sizes[i + 1]))  # Set to zeros if mismatch\n",
    "        except Exception as e:\n",
    "            print(f\"Error while processing weights at layer {i}: {e}\")\n",
    "            weight_array = np.zeros((layer_sizes[i], layer_sizes[i + 1]))  # Set to zeros if error occurs\n",
    "\n",
    "        try:\n",
    "            bias_array = np.zeros(layer_sizes[i + 1]) if biases is None else np.array(biases[i])\n",
    "            # Ensure bias_array is one-dimensional and has the correct shape\n",
    "            if bias_array.ndim != 1 or bias_array.shape != (layer_sizes[i + 1],):\n",
    "                print(f\"Warning: Bias shape mismatch at layer {i}, expected shape ({layer_sizes[i + 1]}) but got {bias_array.shape}\")\n",
    "                bias_array = np.zeros(layer_sizes[i + 1])  # Set to zeros if mismatch\n",
    "        except Exception as e:\n",
    "            print(f\"Error while processing biases at layer {i}: {e}\")\n",
    "            bias_array = np.zeros(layer_sizes[i + 1])  # Set to zeros if error occurs\n",
    "\n",
    "        # Debug: Print the shapes of weights and biases\n",
    "        print(f\"Layer {i} - weights shape: {weight_array.shape}, biases shape: {bias_array.shape}\")\n",
    "\n",
    "        # Set the weights and biases to the layer\n",
    "        model.layers[i].set_weights([weight_array, bias_array])\n",
    "\n",
    "    # Add output layer\n",
    "    model.add(layers.Dense(2, activation='softmax'))  # Example output layer for binary classification\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Example loading models from pickle files\n",
    "model_files = ['trained_model.pkl', 'trained_model1.pkl', 'trained_model2.pkl', 'trained_model3.pkl']\n",
    "models_data = [load_pickle(file) for file in model_files]\n",
    "\n",
    "# Inspect the structure of the first model\n",
    "inspect_pickle_structure(models_data[0])\n",
    "\n",
    "# Create the models using the loaded data\n",
    "models = [create_model_from_pickle(model_data) for model_data in models_data]\n",
    "\n",
    "# Compile and evaluate the models\n",
    "for idx, model in enumerate(models):\n",
    "    if model:\n",
    "        print(f\"Evaluating Model {idx + 1}...\")\n",
    "        # Dummy dataset for example purposes\n",
    "        X, y = np.random.rand(1000, models_data[0]['parameters']['layer_size'][0]), np.random.randint(0, 2, 1000)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # One-hot encode the labels\n",
    "        y_train = to_categorical(y_train, num_classes=2)\n",
    "        y_test = to_categorical(y_test, num_classes=2)\n",
    "\n",
    "        model.fit(X_train, y_train, epochs=10, batch_size=64)\n",
    "        y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "        print(f\"Model {idx + 1} Classification Report:\\n{classification_report(y_test.argmax(axis=1), y_pred.argmax(axis=1))}\")\n",
    "    else:\n",
    "        print(f\"Model {idx + 1} could not be created due to errors.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
