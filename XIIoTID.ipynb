{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from scipy import sparse\n",
    "import pickle\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(r'P:\\\\project ides for final ptoject\\\\project\\\\IDS_Federated learning model\\\\model\\\\cleaned_x_iiotid_dataset.csv', low_memory=False)\n",
    "\n",
    "# Specify the columns to keep\n",
    "columns_to_keep = ['Scr_port', 'Des_port', 'Protocol', 'Service', 'Scr_bytes', 'Des_bytes', 'Conn_state', 'anomaly_alert', 'class3']\n",
    "data_filtered = data[columns_to_keep]\n",
    "\n",
    "# Define features and labels\n",
    "X = data_filtered.drop(columns=['class3'])\n",
    "y = data_filtered['class3']\n",
    "\n",
    "# Handle categorical and numerical columns\n",
    "categorical_columns = ['Protocol', 'Service', 'Conn_state']\n",
    "numerical_columns = X.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', StandardScaler(), numerical_columns),\n",
    "    ('cat', OneHotEncoder(sparse_output=False), categorical_columns)\n",
    "])\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Preprocess the data\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "# Convert to dense format if sparse\n",
    "X_train = X_train.toarray() if sparse.issparse(X_train) else X_train\n",
    "X_test = X_test.toarray() if sparse.issparse(X_test) else X_test\n",
    "\n",
    "# Convert labels to numerical format\n",
    "y_train = pd.factorize(y_train)[0]\n",
    "y_test = pd.factorize(y_test)[0]\n",
    "\n",
    "# One-hot encode if multi-class classification\n",
    "num_classes = len(np.unique(y_train))\n",
    "if num_classes > 2:\n",
    "    y_train = np.eye(num_classes)[y_train]\n",
    "    y_test = np.eye(num_classes)[y_test]\n",
    "\n",
    "# Activation functions\n",
    "def leaky_relu(Z, alpha=0.01):\n",
    "    return np.where(Z > 0, Z, alpha * Z)\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "def softmax(Z):\n",
    "    exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
    "    return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n",
    "\n",
    "# Initialize weights\n",
    "def initialize_weights(layers):\n",
    "    np.random.seed(42)\n",
    "    weights = []\n",
    "    biases = []\n",
    "    for i in range(1, len(layers)):\n",
    "        weight = np.random.randn(layers[i-1], layers[i]) * np.sqrt(2 / layers[i-1])\n",
    "        bias = np.zeros((1, layers[i]))\n",
    "        weights.append(weight)\n",
    "        biases.append(bias)\n",
    "    return weights, biases  # Return as lists, not NumPy arrays\n",
    "\n",
    "# Forward propagation\n",
    "def forward_propagation(X, weights, biases):\n",
    "    A = X\n",
    "    caches = []\n",
    "    for i in range(len(weights) - 1):\n",
    "        Z = np.dot(A, weights[i]) + biases[i]\n",
    "        A = leaky_relu(Z)\n",
    "        caches.append((A, Z))\n",
    "    Z_final = np.dot(A, weights[-1]) + biases[-1]\n",
    "    A_final = softmax(Z_final) if num_classes > 2 else sigmoid(Z_final)\n",
    "    caches.append((A_final, Z_final))\n",
    "    return A_final, caches\n",
    "\n",
    "# Backward propagation\n",
    "def backward_propagation(X_batch, y_true, caches, weights, biases, learning_rate):\n",
    "    m = X_batch.shape[0]\n",
    "    A_final, Z_final = caches[-1]\n",
    "    y_true = np.eye(num_classes)[y_true] if len(y_true.shape) == 1 else y_true\n",
    "\n",
    "    dZ_final = A_final - y_true\n",
    "    dW_final = np.dot(caches[-2][0].T, dZ_final) / m\n",
    "    db_final = np.sum(dZ_final, axis=0, keepdims=True) / m\n",
    "    weights[-1] -= learning_rate * dW_final\n",
    "    biases[-1] -= learning_rate * db_final\n",
    "\n",
    "    dA = dZ_final\n",
    "    for i in reversed(range(len(weights) - 1)):\n",
    "        A_prev, Z_prev = caches[i]\n",
    "        dZ = dA.dot(weights[i+1].T) * (Z_prev > 0)\n",
    "        dW = np.dot(caches[i-1][0].T, dZ) / m if i > 0 else np.dot(X_batch.T, dZ) / m\n",
    "        db = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "        weights[i] -= learning_rate * dW\n",
    "        biases[i] -= learning_rate * db\n",
    "        dA = dZ\n",
    "\n",
    "    return weights, biases\n",
    "\n",
    "# Training the model\n",
    "def train(X_train, y_train, layers, epochs=30, batch_size=64, learning_rate=0.001):\n",
    "    weights, biases = initialize_weights(layers)\n",
    "    m = X_train.shape[0]\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, m, batch_size):\n",
    "            X_batch = X_train[i:i + batch_size]\n",
    "            y_batch = y_train[i:i + batch_size]\n",
    "            y_batch = np.eye(num_classes)[y_batch] if len(y_batch.shape) == 1 else y_batch\n",
    "            y_pred, caches = forward_propagation(X_batch, weights, biases)\n",
    "            weights, biases = backward_propagation(X_batch, y_batch, caches, weights, biases, learning_rate)\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    return weights, biases\n",
    "\n",
    "# Save weights and biases to a pickle file\n",
    "def save_weights_to_pickle(weights, biases, filename):\n",
    "    # Store the weights and biases as numpy arrays inside a dictionary\n",
    "    data = {\n",
    "        \"weights\": [np.array(w) for w in weights],  # Convert list of arrays to numpy arrays\n",
    "        \"biases\": [np.array(b) for b in biases],    # Convert list of arrays to numpy arrays\n",
    "    }\n",
    "    \n",
    "    # Save the dictionary to a pickle file\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(data, file)\n",
    "    print(f\"Weights and biases saved to {filename}\")\n",
    "\n",
    "# Load weights and biases from a pickle file\n",
    "def load_weights_from_pickle(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    weights = data[\"weights\"]\n",
    "    biases = data[\"biases\"]\n",
    "    return weights, biases\n",
    "\n",
    "# Define layers and train the model\n",
    "input_size = X_train.shape[1]\n",
    "layers = [input_size, 512, 256, 128, num_classes]\n",
    "weights, biases = train(X_train, y_train, layers, epochs=30, batch_size=64, learning_rate=0.001)\n",
    "\n",
    "# Save weights and biases\n",
    "save_weights_to_pickle(weights, biases, \"model_weights_and_biases.pickle\")\n",
    "\n",
    "# Load weights and biases (example usage)\n",
    "loaded_weights, loaded_biases = load_weights_from_pickle(\"model_weights_and_biases.pickle\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred, _ = forward_propagation(X_test, loaded_weights, loaded_biases)\n",
    "\n",
    "# Convert predictions back to labels\n",
    "y_pred_labels = np.argmax(y_pred, axis=1) if num_classes > 2 else (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Calculate and print metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_labels)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_labels)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
